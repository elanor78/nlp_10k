# -*- coding: utf-8 -*-
"""part1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DSB_OknLFJeHKGmr3wC_HFE7ICV_x919
"""

import streamlit as st
import json
import os
import pandas as pd
import numpy as np
from transformers import pipeline
import tensorflow as tf
import plotly.express as px
import plotly.graph_objects as go
import subprocess


def download_10k_filings(ticker, start_year):
    """
    Downloads and extracts 10-K filings for the specified ticker and starting year until the current year.
    If the required files are not found locally, it will trigger the download process.
    Args:
        ticker (str): The stock ticker symbol for the company (e.g., "AAPL").
        start_year (int): The year from which to start downloading filings.
    """
    from datetime import datetime
    current_year = datetime.now().year

    repo_url = "https://github.com/nlpaueb/edgar-crawler.git"
    repo_dir = "edgar-crawler"

    if not os.path.exists(repo_dir):
        print(f"Cloning the repository from {repo_url}...")
        subprocess.run(["git", "clone", repo_url], check=True)

    requirements_file = os.path.join(repo_dir, "requirements.txt")
    if not os.path.exists(requirements_file):
        print(f"Downloading {requirements_file}...")
        subprocess.run(["curl", "-O", "https://raw.githubusercontent.com/nlpaueb/edgar-crawler/master/requirements.txt"], check=True)

    print("Installing required dependencies...")
    subprocess.run(["pip", "install", "-r", requirements_file], check=True)

    config = {
        "download_filings": {
            "start_year": start_year,
            "end_year": current_year,
            "quarters": [1, 2, 3, 4],
            "filing_types": ["10-K"],
            "cik_tickers": [ticker],
            "user_agent": "Your Name (your-email@example.com)",
            "raw_filings_folder": "RAW_FILINGS",
            "indices_folder": "INDICES",
            "filings_metadata_file": "FILINGS_METADATA.csv",
            "skip_present_indices": True
        },
        "extract_items": {
            "raw_filings_folder": "RAW_FILINGS",
            "extracted_filings_folder": "EXTRACTED_FILINGS",
            "filings_metadata_file": "FILINGS_METADATA.csv",
            "filing_types": ["10-K"],
            "include_signature": False,
            "items_to_extract": [],
            "remove_tables": True,
            "skip_extracted_filings": True
        }
    }

    with open(os.path.join(repo_dir, 'config.json'), 'w') as f:
        json.dump(config, f, indent=4)

    try:
        print(f"Downloading filings for {ticker} from {start_year} to {current_year}...")
        subprocess.run(["python", os.path.join(repo_dir, "download_filings.py")], check=True)
        print(f"Extracting items from filings for {ticker}...")
        subprocess.run(["python", os.path.join(repo_dir, "extract_items.py")], check=True)
        print("Process completed successfully!")
    except subprocess.CalledProcessError as e:
        print(f"An error occurred: {e}")

    """
    Downloads and extracts 10-K filings for the specified ticker and starting year until the current year.
    This function will clone the EDGAR crawler repository, download the requirements.txt if it's not present,
    set up configurations, and run the download and extract scripts.

    Args:
        ticker (str): The stock ticker symbol for the company (e.g., "AAPL").
        start_year (int): The year from which to start downloading filings.
    """
    from datetime import datetime
    current_year = datetime.now().year

    repo_url = "https://github.com/nlpaueb/edgar-crawler.git"
    repo_dir = "edgar-crawler"

    if not os.path.exists(repo_dir):
        print(f"Cloning the repository from {repo_url}...")
        subprocess.run(["git", "clone", repo_url], check=True)

    os.chdir(repo_dir)

    requirements_file = "requirements.txt"
    if not os.path.exists(requirements_file):
        print(f"Downloading {requirements_file}...")
        subprocess.run(["curl", "-O", "https://raw.githubusercontent.com/nlpaueb/edgar-crawler/master/requirements.txt"], check=True)

    print("Installing required dependencies...")
    subprocess.run(["pip", "install", "-r", requirements_file], check=True)

    config = {
        "download_filings": {
            "start_year": start_year,
            "end_year": current_year,
            "quarters": [1, 2, 3, 4],
            "filing_types": ["10-K"],
            "cik_tickers": [ticker],
            "user_agent": "Your Name (your-email@example.com)",
            "raw_filings_folder": "RAW_FILINGS",
            "indices_folder": "INDICES",
            "filings_metadata_file": "FILINGS_METADATA.csv",
            "skip_present_indices": True
        },
        "extract_items": {
            "raw_filings_folder": "RAW_FILINGS",
            "extracted_filings_folder": "EXTRACTED_FILINGS",
            "filings_metadata_file": "FILINGS_METADATA.csv",
            "filing_types": ["10-K"],
            "include_signature": False,
            "items_to_extract": [],
            "remove_tables": True,
            "skip_extracted_filings": True
        }
    }

    with open('config.json', 'w') as f:
        json.dump(config, f, indent=4)

    try:
        print(f"Downloading filings for {ticker} from {start_year} to {current_year}...")
        subprocess.run(["python", "download_filings.py"], check=True)

        print(f"Extracting items from filings for {ticker}...")
        subprocess.run(["python", "extract_items.py"], check=True)

        print("Process completed successfully!")
    except subprocess.CalledProcessError as e:
        print(f"An error occurred: {e}")
    finally:
        os.chdir('..')


sentiment_pipeline = pipeline("sentiment-analysis", model="yiyanghkust/finbert-tone")

def analyze_sentiment(text):
    result = sentiment_pipeline(text)[0]
    return result['score'] if result['label'] == 'positive' else 0


def extract_all_json_content(folder_path):
    extracted_content = []

    if not os.path.exists(folder_path):
        print(f"Error: The folder '{folder_path}' does not exist.")
        return extracted_content

    for file_name in os.listdir(folder_path):
        if file_name.endswith(".json"):
            file_path = os.path.join(folder_path, file_name)

            try:
                parts = file_name.replace(".json", "").split("_")[:3]
                if len(parts) < 3:
                    print(f"Skipping invalid filename: {file_name}")
                    continue

                cik, filing_type, year = parts

                with open(file_path, 'r') as f:
                    content = json.load(f)

                content["cik"] = cik
                content["filing_type"] = filing_type
                content["year"] = year

                extracted_content.append(content)
                print(f"Successfully extracted data from {file_name}")
            except Exception as e:
                print(f"Error reading {file_name}: {e}")

    return extracted_content


st.title("EDGAR 10-K Filings Sentiment Analysis")
ticker = st.text_input("Enter Ticker Symbol:")
year = st.number_input("Enter Start Year:", min_value=2000, max_value=2025, step=1)

if st.button("Analyze"):
    if ticker and year:
        download_10k_filings(ticker, year)
        folder_path = f"edgar-crawler/EXTRACTED_FILINGS/10-K"
        data = extract_all_json_content(folder_path)

        if not data:
            st.error("No data found. Ensure the EDGAR crawler ran correctly.")
        else:
            company_dfs = {}
            for report in data:
                company_name = report.get('company', 'Unknown')
                report_year = report.get('year', 'Unknown')
                if company_name not in company_dfs:
                    company_dfs[company_name] = pd.DataFrame(columns=['year'] + [f'item_{i}' for i in range(1, 17)])
                row = {'year': report_year}
                for item in range(1, 17):
                    item_key = f'item_{item}'
                    row[item_key] = analyze_sentiment(report.get(item_key, ''))
                company_dfs[company_name] = pd.concat([company_dfs[company_name], pd.DataFrame([row])], ignore_index=True)

            for company, df in company_dfs.items():
                st.subheader(f"Sentiment Scores for {company}")
                st.dataframe(df)
                df['Average'] = df.mean(axis=1)
                fig = px.line(df, x='year', y='Average', title=f"Sentiment Over Time for {company}")
                st.plotly_chart(fig)
    else:
        st.error("Please enter both Ticker Symbol and Start Year.")
    if ticker and year:
        folder_path = f"edgar-crawler/datasets/EXTRACTED_FILINGS/10-K"
        data = extract_all_json_content(folder_path)

        for report in data:
            st.write(report)

    else:
        st.error("Please enter both Ticker Symbol and Start Year.")


